title: Ph.D. Thesis @ UNC
date: 2008-08-01

**Clique: Perceptually Based, Task Oriented Auditory Display for GUI Applications**

Screen reading is the prevalent approach for presenting graphical desktop applications in audio. The primary function of a screen reader is to describe widgets the user encounters when interacting with a graphical user interface (GUI). This straightforward method allows people with visual impairments to hear exactly what is on the screen, but with significant usability problems in a multitasking environment. Screen reader users must infer the state of on-going tasks spanning multiple graphical windows, from a single, serial stream of speech describing one widget after another.

In this dissertation, I explore a new approach to enabling auditory display of GUI programs. With this method, the display describes concurrent application tasks using a small set of simultaneous speech and sound streams. The user listens to and interacts solely with this display, never with the underlying graphical interfaces. Scripts support this level of adaption by mapping GUI widgets to task definitions. Evaluation of this approach shows improvements in user efficiency, satisfaction, and understanding with relatively little development effort.

To develop this method, I studied the literature on existing auditory displays, working user behavior, and theories of human auditory perception and processing. I then conducted a user study to observe problems encountered and techniques employed by users interacting with an ideal auditory display: another human being. Based on my findings, I designed and implemented a prototype auditory display, called Clique, along with scripts adapting seven GUI applications. I concluded my work by conducting a variety of evaluations on Clique. The results of these studies show the following benefits of Clique over the state of the art for users with visual impairments (1-5) and mobile sighted users (6):

1. Faster, accurate access to speech utterances through concurrent speech streams.
2. Better awareness of peripheral information via concurrent speech and sound streams.
3. Increased information bandwidth through concurrent streams.
4. More efficient information seeking enabled by ubiquitous tools for browsing and searching.
5. Greater accuracy in describing unfamiliar applications learned using a consistent, task-based user interface.
6. Faster completion of email tasks in a standard GUI after exposure to those tasks in audio.

## Documents

* Parente, Peter. [Clique: Perceptually Based, Task Oriented Auditory Display for GUI Applications](https://s3.amazonaws.com/mindtrove/clique/parente-clique.pdf) Ph.D. Thesis. University of North Carolina-Chapel Hill. July, 2008.
* The [proposal document](https://s3.amazonaws.com/mindtrove/clique/parente-proposal.pdf) introducing Clique as my dissertation topic approved in December, 2004.

## Example Movie

The following video gives a sample of the Clique user experience. In the video, a user works to complete a task assigned by email using multiple programs. The speech and sounds heard are all generated by Clique, and all changes in the visual GUIs are performed by Clique as it carries out the user commands. The captions in the video explain what the user is currently doing.

An accessible alternative to the Flash player embedded below is also available. [Click here to download and automatically play the movie in Quicktime](https://s3.amazonaws.com/mindtrove/clique/clique.mov).

<div class="centered">
<iframe src="//player.vimeo.com/video/1206595" width="640" height="480" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>

## Example Sounds

The following sounds are examples of various concepts described in the dissertation document.

<table><caption>Table of example sounds in OGG and MP3 formats</caption>
<thead>
<tr>
<th>Description</th>
<th>Reference</th>
<th>Audio</th>
</tr>
</thead>
<tbody>
<tr>
<td>Concatenative speech synthesis</td>
<td>Chapter 2, Section 2.1.1, Page 16</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/concatenative.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/concatenative.mp3">MP3</a></td>
</tr>
<tr>
<td>Formant speech synthesis</td>
<td>Chapter 2, Section 2.1.1, Page 17</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/formant.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/formant.mp3">MP3</a></td>
</tr>
<tr>
<td>Auditory icons</td>
<td>Chapter 2, Section 2.1.2, Page 18</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/auditoryicons.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/auditoryicons.mp3">MP3</a></td>
</tr>
<tr>
<td>Familial earcons</td>
<td>Chapter 2, Section 2.1.3, Page 20</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/earcons.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/earcons.mp3">MP3</a></td>
</tr>
<tr>
<td>Ambient sound</td>
<td>Chapter 2, Section 2.1.4, Page 22</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/ambient.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/ambient.mp3">MP3</a></td>
</tr>
<tr>
<td>Audio mixing</td>
<td>Chapter 2, Section 2.1.5, Page 23</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/mixing.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/mixing.mp3">MP3</a></td>
</tr>
<tr>
<td>HRTF spatialized sound</td>
<td>Chapter 2, Section 2.1.6, Page 24</td>
<td><a href="http://eamusic.dartmouth.edu/~corey/cmj_sound_ex/cmj_sound_ex.html">External link</a></td>
</tr>
<tr>
<td>Screen reading a Web page</td>
<td>Chapter 2, Sections 2.5.2 and 2.5.3, Pages 60-67</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/screenreader.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/screenreader.mp3">MP3</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/cs-homepage.png">Screenshot</a></td>
</tr>
<tr>
<td>Ideal display interaction</td>
<td>Chapter 3, Section 3.2.1, Page 91, List item #3</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/ideal.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/ideal.mp3">MP3</a></td>
</tr>
<tr>
<td>Temporal stream integration</td>
<td>Chapter 4, Section 4.1.2, Pages 133-134</td>
<td><a href="http://www.psych.mcgill.ca/labs/auditory/bregmancd.html#listof">External Link</a></td>
</tr>
<tr>
<td>Spectral stream integration</td>
<td>Chapter 4, Section 4.1.2, Pages 133-134</td>
<td><a href="http://www.psych.mcgill.ca/labs/auditory/bregmancd.html#listof">External Link</a></td>
</tr>
<tr>
<td>Content assistant in isolation</td>
<td>Chapter 5, Section 5.1.1, Pages 160-165</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/content.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/content.mp3">MP3</a></td>
</tr>
<tr>
<td>Summary assistant in isolation</td>
<td>Chapter 5, Section 5.1.1, Pages 160-165</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/summary.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/summary.mp3">MP3</a></td>
</tr>
<tr>
<td>Related assistant in isolation</td>
<td>Chapter 5, Section 5.1.1, Pages 160-165</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/related.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/related.mp3">MP3</a></td>
</tr>
<tr>
<td>Environmental sound theme in isolation</td>
<td>Chapter 5, Section 5.1.2, Pages 165-168</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/theme.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/theme.mp3">MP3</a></td>
</tr>
<tr>
<td>Program menu</td>
<td>Chapter 5, Section 5.1.3, Pages 168-169</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/programmenu.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/programmenu.mp3">MP3</a></td>
</tr>
<tr>
<td>Task menu</td>
<td>Chapter 5, Section 5.1.3, Pages 168-169</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/taskmenu.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/taskmenu.mp3">MP3</a></td>
</tr>
<tr>
<td>Assistant response to task navigation</td>
<td>Chapter 5, Section 5.1.4, Pages 170-171</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/tasknav2.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/tasknav2.mp3">MP3</a></td>
</tr>
<tr>
<td>Assistant response to content browsing</td>
<td>Chapter 5, Section 5.1.5, Page 173</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/browse.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/browse.mp3">MP3</a></td>
</tr>
<tr>
<td>Assistant response to content searching</td>
<td>Chapter 5, Section 5.1.5, Page 173</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/search.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/search.mp3">MP3</a></td>
</tr>
<tr>
<td>Assistant response to content editing</td>
<td>Chapter 5, Section 5.1.5, Page 173</td>
<td><a href="https://s3.amazonaws.com/mindtrove/clique/edit.ogg">OGG</a>, <a href="https://s3.amazonaws.com/mindtrove/clique/edit.mp3">MP3</a></td>
</tr>
</tbody>
</table>

## Source Code

The [Clique source code](https://github.com/parente/clique) is  [BSD licensed](http://www.opensource.org/licenses/bsd-license.php). I provide it as a reference implementation of a task-based, multichannel auditory display in hope that developers will revise and extend its core concepts.

The source includes the following sounds licensed under various [Creative Commons licenses](http://creativecommons.org/):

* By <a href="http://freesound.iua.upf.edu/usersViewSingle.php?id=14447">man</a>: <a href="http://freesound.iua.upf.edu/samplesViewSingle.php?id=14624">soldati-marcia.aif</a>
* By <a href="http://freesound.iua.upf.edu/usersViewSingle.php?id=29612">jnr hacksaw</a>: <a href="http://freesound.iua.upf.edu/samplesViewSingle.php?id=11221">Zap.flac</a>
* By <a href="http://freesound.iua.upf.edu/usersViewSingle.php?id=197070">csengeri</a>: <a href="http://freesound.iua.upf.edu/samplesViewSingle.php?id=34218">Cricket2.wav</a>


## Acknowledgement

This material is based upon work supported under a National Science Foundation Graduate Research Fellowship. Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation